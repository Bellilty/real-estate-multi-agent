## ğŸ—ï¸ System Architecture

### Overview

The Real Estate Multi-Agent System uses **LangGraph** to orchestrate multiple specialized AI agents that work together to process user queries about real estate data. The system features a clean separation between frontend, backend, and AI components, with built-in Chain-of-Thought reasoning and performance tracking.

### ğŸ“Š Data Source

**Primary Dataset:** `data/cortex.parquet`

The system retrieves all financial information from a **Parquet dataset** containing real estate transactions:

- **Format:** Apache Parquet (columnar, compressed)
- **Records:** 3,924 financial transactions
- **Properties:** 5 buildings (Building 17, 120, 140, 160, 180)
- **Entity:** PropCo (Property Company)
- **Time Period:** 2024-01 to 2025-03
- **Data Fields:**
  - `entity_name`: Organization (PropCo)
  - `property_name`: Building identifier
  - `tenant_name`: Tenant occupying the space
  - `ledger_type`: revenue or expenses
  - `ledger_category`: Rent, parking, mortgage, etc.
  - `profit`: Transaction amount
  - `year`, `quarter`, `month`: Time dimensions

**Data Loading:** Polars library (Python) for high-performance data operations

**Why This Approach?**
- âœ… No external API dependencies (works offline)
- âœ… Fast queries (<10ms for most operations)
- âœ… Easy to version control and replicate
- âœ… Industry-standard format for analytics

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USER INTERFACE                        â”‚
â”‚              (Streamlit - frontend/)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ORCHESTRATOR (backend/core/)                â”‚
â”‚         Coordinates agents with LangGraph                â”‚
â”‚         + Chain-of-Thought Tracking                      â”‚
â””â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”œâ”€â”€â”€â”€â–º Router Agent â”‚â”€â”€â”€â–ºâ”‚ Extractor    â”‚
  â”‚    â”‚ (Intent)     â”‚    â”‚ (Entities)   â”‚
  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”œâ”€â”€â”€â”€â–º Query Agent  â”‚â”€â”€â”€â–ºâ”‚ Formatter    â”‚
  â”‚    â”‚ (Data)       â”‚    â”‚ (Response)   â”‚
  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â””â”€â”€â”€â”€â–º Fallback     â”‚
       â”‚ (Errors)     â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   DATA LAYER        â”‚
  â”‚  (Polars DataFrame) â”‚
  â”‚  cortex.parquet     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
Cortex-multi-agent-task/
â”‚
â”œâ”€â”€ frontend/                 # ğŸ¨ Frontend Layer
â”‚   â””â”€â”€ streamlit_app.py     # Streamlit UI with Chain-of-Thought display
â”‚
â”œâ”€â”€ backend/                  # âš™ï¸ Backend Layer (Complete)
â”‚   â”œâ”€â”€ core/                # Core orchestration logic
â”‚   â”‚   â””â”€â”€ orchestrator.py  # LangGraph workflow coordinator
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/              # ğŸ¤– AI Agents
â”‚   â”‚   â”œâ”€â”€ router_v2.py     # Intent classification
â”‚   â”‚   â”œâ”€â”€ extractor_v2.py  # Entity extraction with fallbacks
â”‚   â”‚   â”œâ”€â”€ query_v2.py      # Data query execution
â”‚   â”‚   â””â”€â”€ formatter_v2.py  # Response formatting
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                # ğŸ“Š Data Layer
â”‚   â”‚   â””â”€â”€ data_loader.py   # Polars data operations
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/                 # ğŸ¤– LLM Layer
â”‚   â”‚   â””â”€â”€ llm_client.py    # HuggingFace LLM wrapper
â”‚   â”‚
â”‚   â””â”€â”€ utils/               # ğŸ› ï¸ Utilities
â”‚       â”œâ”€â”€ tracking.py      # Performance & Chain-of-Thought tracking
â”‚       â””â”€â”€ prompts.py       # Enhanced prompt templates
â”‚
â”œâ”€â”€ data/                     # ğŸ“Š Dataset
â”‚   â””â”€â”€ cortex.parquet       # Real estate financial data
â”‚
â”œâ”€â”€ .env.local               # ğŸ” API keys (not in git)
â”œâ”€â”€ requirements.txt         # ğŸ“¦ Python dependencies
â””â”€â”€ run.sh                   # ğŸš€ Launch script
```

---

## ğŸ”„ Data Flow: From Question to Answer

### Step-by-Step Process

#### **1. User Input** 
```
User: "Compare Building 140 to Building 180"
```

#### **2. Router Agent** (Intent Classification)
- **Input:** Raw user query
- **Process:** Sends query to LLM with classification prompt
- **Output:** Intent type + confidence level
- **Example:**
  ```
  Intent: property_comparison
  Confidence: high
  Reason: Two buildings to compare
  ```

#### **3. Extractor Agent** (Entity Extraction)
- **Input:** User query + Intent
- **Process:** 
  - Uses intent-specific prompt to extract entities
  - Applies regex fallbacks if LLM fails
  - Validates against available properties
- **Output:** Extracted entities
- **Example:**
  ```json
  {
    "properties": ["Building 140", "Building 180"],
    "count": 2
  }
  ```

#### **4. Query Agent** (Data Retrieval)
- **Input:** Intent + Entities
- **Process:**
  - Validates entities against dataset
  - Executes Polars queries on `cortex.parquet`
  - Performs calculations (P&L, comparisons, etc.)
- **Output:** Raw data results
- **Example:**
  ```json
  {
    "Building 140": {
      "total_revenue": 537340.10,
      "total_expenses": 10681.25,
      "net_profit": 526658.85
    },
    "Building 180": {
      "total_revenue": 391490.26,
      "total_expenses": 6590.23,
      "net_profit": 384900.03
    }
  }
  ```

#### **5. Formatter Agent** (Response Generation)
- **Input:** User query + Data results + Chain of thought summary
- **Process:** Sends data to LLM with formatting prompt
- **Output:** Natural language response in Markdown
- **Example:**
  ```markdown
  ğŸ¢ **Property Comparison: Building 140 vs Building 180**
  
  | Metric | Building 140 | Building 180 |
  |--------|-------------|--------------|
  | Total Revenue | â‚¬537,340.10 | â‚¬391,490.26 |
  | Net Profit | â‚¬526,658.85 | â‚¬384,900.03 |
  
  Building 140 has â‚¬141,758.82 higher profit.
  ```

#### **6. Chain-of-Thought Display**
The system tracks each step with:
- Agent name
- Action performed
- Reasoning
- Execution time (ms)
- Success/failure status

---

## ğŸ§  Chain-of-Thought Tracking

The system implements a transparent reasoning process:

```python
class ChainOfThoughtTracker:
    - Tracks each agent's reasoning
    - Measures execution time per step
    - Counts LLM API calls
    - Provides performance metrics
```

**Example Output:**
```
1. Router (1273ms): Classified as 'property_comparison' with high confidence
2. Extractor (721ms): Extracted 2 properties: Building 140, Building 180
3. Query (9ms): Retrieved comparison data
4. Formatter (2177ms): Generated 709 character response

Total: 4234ms | LLM Calls: 3
```

---

## ğŸ¯ Enhanced Features

### 1. **Robust Entity Extraction**
- LLM-based extraction with **regex fallbacks**
- Handles partial matches ("140" â†’ "Building 140")
- Validates against available data

### 2. **Error Handling**
- Validates all entities before querying
- Provides helpful error messages with available options
- Fallback agent for unsupported requests

### 3. **Performance Optimization**
- Tracks execution time per agent
- Monitors LLM API usage
- Provides performance metrics to user

### 4. **Flexible Prompts**
- Intent-specific prompt templates
- Few-shot examples for better accuracy
- Structured output format for reliable parsing

---

## ğŸ”§ Technology Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Frontend** | Streamlit | Interactive web UI |
| **Orchestration** | LangGraph | Multi-agent workflow |
| **LLM** | Llama 3.2-3B (HuggingFace) | Natural language processing |
| **Data** | Polars | Fast dataframe operations |
| **State Management** | TypedDict | Type-safe state passing |
| **Tracking** | Custom Tracker | Chain-of-Thought & metrics |

---

## ğŸš€ Performance Characteristics

| Operation | Typical Time | LLM Calls |
|-----------|-------------|-----------|
| Property Comparison | ~4-5s | 3 |
| P&L Calculation | ~3-4s | 3 |
| Property Details | ~3-4s | 3 |
| Error Fallback | <1s | 1 |

*Note: Times vary based on LLM API response time*

---

## ğŸ›¡ï¸ Error Handling Strategy

### 1. **Validation Errors**
```
Property not found â†’ Show available properties
No data for time period â†’ Suggest valid periods
```

### 2. **Extraction Failures**
```
LLM fails to extract â†’ Regex fallback
No entities found â†’ Request clarification
```

### 3. **LLM API Errors**
```
Rate limit â†’ Graceful error message
Network error â†’ Retry logic
```

### 4. **Unsupported Queries**
```
Intent: unsupported â†’ Fallback agent
Shows: Capabilities + Examples
```

---

## ğŸ“Š Data Model

### Input Dataset (cortex.parquet)
```
Columns:
- period: Date (YYYY-MM)
- property_name: Building identifier
- tenant_name: Tenant identifier  
- ledger_type: revenue/expense
- ledger: Specific type (rent, mortgage, etc.)
- profit: Amount (â‚¬)
```

### Agent State (WorkflowState)
```python
{
    "user_query": str,      # Original question
    "intent": str,          # Classified intent
    "confidence": str,      # Classification confidence
    "entities": dict,       # Extracted parameters
    "query_result": dict,   # Data from database
    "final_response": str,  # Generated answer
    "tracker": Tracker      # Reasoning & metrics
}
```

---

## ğŸ¨ UI Features

1. **Query Input** - Natural language input field
2. **Example Queries** - One-click pre-populated questions
3. **Response Display** - Formatted markdown output
4. **Chain-of-Thought** - Expandable reasoning view
5. **Performance Metrics** - Time, LLM calls, steps
6. **Query History** - Last 5 queries with replay
7. **Dataset Info** - Properties, tenants, date range

---

## ğŸ§ª Testing & Validation

The system has been tested with:

âœ… Property comparisons (2 properties)
âœ… P&L calculations (with/without filters)
âœ… Property details queries
âœ… Tenant information queries
âœ… Non-existent properties (error handling)
âœ… Vague queries (fallback)
âœ… Edge cases (partial names, typos)

---

## ğŸ”® Future Enhancements

1. **Caching** - Cache frequent queries
2. **Batch Processing** - Handle multiple queries at once
3. **Advanced Analytics** - Trend analysis, forecasting
4. **Multi-property Comparison** - Compare 3+ properties
5. **Natural Language Filters** - "Last quarter", "This year"
6. **Export** - Download results as CSV/PDF

---

## ğŸ“š Code Quality

- âœ… Type hints throughout
- âœ… Comprehensive docstrings
- âœ… Clean separation of concerns
- âœ… Error handling at every layer
- âœ… Performance monitoring
- âœ… Logging and debugging support

---

## ğŸ“ Key Design Decisions

### Why LangGraph?
- **Declarative workflow** - Easy to visualize and modify
- **State management** - Clean data passing between agents
- **Conditional edges** - Dynamic routing based on results

### Why Polars?
- **Fast** - 5-10x faster than Pandas
- **Memory efficient** - Lazy evaluation
- **SQL-like** - Familiar query syntax

### Why Llama 3.2-3B?
- **Free** - HuggingFace Inference API
- **Fast** - Small model = quick responses  
- **Good enough** - Sufficient for classification/extraction

### Why Chain-of-Thought?
- **Transparency** - Users see reasoning process
- **Debugging** - Easy to identify failure points
- **Trust** - Build confidence in AI decisions

---

## ğŸ’¡ Best Practices Implemented

1. **Separation of Concerns**: Frontend / Backend / AI layers
2. **Fail Fast**: Validate early, provide clear errors
3. **Fallback Logic**: Always have a backup plan
4. **User Feedback**: Show progress, reasoning, and metrics
5. **Clean Code**: Type hints, docstrings, consistent naming
6. **Performance Tracking**: Measure everything
7. **Structured Prompts**: Few-shot examples, clear formats
8. **Robust Extraction**: LLM + regex fallbacks

---

*For setup instructions, see [QUICK_START.md](QUICK_START.md)*
*For full documentation, see [README.md](README.md)*

